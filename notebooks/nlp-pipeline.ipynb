{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end NLP pipeline\n",
    "Steps:\n",
    "1. Load data\n",
    "2. Preprocess data\n",
    "   - Tokenize\n",
    "   - Vectorize\n",
    "   - Zeropad sequences\n",
    "   - Split data into train and test set\n",
    "3. Build and train model\n",
    "4. Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['KERAS_BACKEND']='tensorflow'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('../data/enron_with_categories/categories.txt', mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files_in_directory(source_directory, file_ext):\n",
    "    categories = {}\n",
    "    for i, file in enumerate(os.listdir(source_dir)):\n",
    "        if file_ext in file:\n",
    "            contents = open('{}/{}'.format(source_directory, file), mode='r')\n",
    "            key = int(file.split('.')[0])\n",
    "            categories[key] = contents.read()\n",
    "                    \n",
    "    return categories\n",
    "\n",
    "\n",
    "project_dir = os.getcwd()\n",
    "source_dir = os.path.join(project_dir, '../data/enron_with_categories/1')\n",
    "categories = load_files_in_directory(source_dir, '.cats')\n",
    "assert len(categories) == 834\n",
    "assert isinstance(categories, dict)\n",
    "assert categories[10425] == \"\"\"1,1,1\n",
    "2,6,1\n",
    "2,13,1\n",
    "3,3,1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def sort_dictionary_by_keys(dictionary):\n",
    "    return OrderedDict(sorted(dictionary.items()))\n",
    "\n",
    "sorted_categories = sort_dictionary_by_keys(categories)\n",
    "assert len(sorted_categories) == len(categories)\n",
    "assert categories[10425] == \"\"\"1,1,1\n",
    "2,6,1\n",
    "2,13,1\n",
    "3,3,1\n",
    "\"\"\"\n",
    "sorted_categories_keys = list(sorted_categories.keys())\n",
    "assert sorted_categories_keys[1] > sorted_categories_keys[0]\n",
    "assert sorted_categories_keys[2] > sorted_categories_keys[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_categories(categories):\n",
    "    # a simple/downsampled implementation of category-labelling for the first iteration\n",
    "    simple_categories = {}\n",
    "    for key, category in categories.items():\n",
    "        simple_categories[key] = category.split(',')[0]\n",
    "        \n",
    "    return simple_categories\n",
    "\n",
    "simple_categories = simplify_categories(categories)\n",
    "assert len(simple_categories) == len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = load_files_in_directory(source_dir, '.cats')\n",
    "simple_categories = simplify_categories(categories)\n",
    "sorted_categories = sort_dictionary_by_keys(simple_categories)\n",
    "\n",
    "emails = load_files_in_directory(source_dir, '.txt')\n",
    "sorted_emails = sort_dictionary_by_keys(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_and_labels(data, labels):\n",
    "\n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "    filenames_list = []\n",
    "    \n",
    "    if (len(data) != len(labels)):\n",
    "        raise Exception('data and labels are of differing length')\n",
    "    \n",
    "    for k, v in data.items():\n",
    "        data_list.append(v)\n",
    "    \n",
    "    for k, v in labels.items():\n",
    "        labels_list.append(v)\n",
    "        \n",
    "    for i in range(len(labels)):\n",
    "        data_key = list(data.items())[i][0]\n",
    "        label_key = list(labels.items())[i][0]\n",
    "        if data_key == label_key:\n",
    "            filenames_list.append(data_key)\n",
    "        else:\n",
    "            raise Exception('data and labels are not sorted in sequence')\n",
    "        \n",
    "    return (data_list, labels_list, filenames_list)\n",
    "\n",
    "emails_list, categories_list, filenames_list = get_data_and_labels(sorted_emails, sorted_categories)\n",
    "assert len(emails_list) == len(categories_list)\n",
    "assert len(emails_list) == len(filenames_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def fit_tokenizer(texts, MAX_NB_WORDS=20000):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='\\\"\\'\\\\')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = fit_tokenizer(emails_list)\n",
    "assert tokenizer.document_count == len (emails_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_texts_to_sequences(texts):\n",
    "    sequences = tokenizer.texts_to_sequences(emails_list)\n",
    "    return sequences\n",
    "\n",
    "sequences = convert_texts_to_sequences(emails_list)\n",
    "assert len(sequences) == len(emails_list)\n",
    "first_word_in_first_email = emails_list[0].split()[0].lower()\n",
    "assert sequences[0][0] == tokenizer.word_index[first_word_in_first_email]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence \n",
    "\n",
    "def zeropad_data(sequences, MAX_SEQUENCE_LENGTH=1000):\n",
    "    data = sequence.pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    return data\n",
    "\n",
    "SEQUENCE_LENGTH_LIMIT=500\n",
    "data = zeropad_data(sequences, SEQUENCE_LENGTH_LIMIT)\n",
    "assert padded_sequences.shape == (len(sequences), SEQUENCE_LENGTH_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tw-ml-template",
   "language": "python",
   "name": "tw-ml-template"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
