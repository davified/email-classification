{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end NLP pipeline\n",
    "Steps:\n",
    "1. Load data\n",
    "2. Preprocess data\n",
    "   - Tokenize\n",
    "   - Vectorize\n",
    "   - Zeropad sequences\n",
    "   - Split data into train and test set\n",
    "3. Build and train model\n",
    "4. Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "os.environ['KERAS_BACKEND']='tensorflow'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files_in_directory(source_directory, file_ext):\n",
    "    categories = {}\n",
    "    for i, file in enumerate(os.listdir(source_dir)):\n",
    "        if file_ext in file:\n",
    "            contents = open('{}/{}'.format(source_directory, file), mode='r')\n",
    "            key = int(file.split('.')[0])\n",
    "            categories[key] = contents.read()\n",
    "                    \n",
    "    return categories\n",
    "\n",
    "\n",
    "project_dir = os.getcwd()\n",
    "source_dir = os.path.join(project_dir, '../data/enron_with_categories/1')\n",
    "categories = load_files_in_directory(source_dir, '.cats')\n",
    "assert len(categories) == 834\n",
    "assert isinstance(categories, dict)\n",
    "assert categories[10425] == \"\"\"1,1,1\n",
    "2,6,1\n",
    "2,13,1\n",
    "3,3,1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def sort_dictionary_by_keys(dictionary):\n",
    "    return OrderedDict(sorted(dictionary.items()))\n",
    "\n",
    "sorted_categories = sort_dictionary_by_keys(categories)\n",
    "assert len(sorted_categories) == len(categories)\n",
    "assert categories[10425] == \"\"\"1,1,1\n",
    "2,6,1\n",
    "2,13,1\n",
    "3,3,1\n",
    "\"\"\"\n",
    "sorted_categories_keys = list(sorted_categories.keys())\n",
    "assert sorted_categories_keys[1] > sorted_categories_keys[0]\n",
    "assert sorted_categories_keys[2] > sorted_categories_keys[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def simplify_categories(categories):\n",
    "    # a simple/downsampled implementation of category-labelling for the first iteration\n",
    "    simple_categories = {}\n",
    "    for key, category in categories.items():\n",
    "        simple_categories[key] = []\n",
    "        categories_in_a_single_email = category.split('\\n')\n",
    "        \n",
    "        for cat in categories_in_a_single_email:\n",
    "            if cat.split(',')[0] != '':\n",
    "                simple_categories[key].append(cat.split(',')[0])\n",
    "                \n",
    "        simple_categories[key] = list(set(simple_categories[key]))\n",
    "        \n",
    "    return simple_categories\n",
    "\n",
    "simple_categories = simplify_categories(categories)\n",
    "assert len(simple_categories) == len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = load_files_in_directory(source_dir, '.cats')\n",
    "simple_categories = simplify_categories(categories)\n",
    "sorted_categories = sort_dictionary_by_keys(simple_categories)\n",
    "\n",
    "emails = load_files_in_directory(source_dir, '.txt')\n",
    "sorted_emails = sort_dictionary_by_keys(emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_and_labels(data, labels):\n",
    "\n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "    filenames_list = []\n",
    "    \n",
    "    if (len(data) != len(labels)):\n",
    "        raise Exception('data and labels are of differing length')\n",
    "    \n",
    "    for k, v in data.items():\n",
    "        data_list.append(v)\n",
    "    \n",
    "    for k, v in labels.items():\n",
    "        labels_list.append(v)\n",
    "        \n",
    "    for i in range(len(labels)):\n",
    "        data_key = list(data.items())[i][0]\n",
    "        label_key = list(labels.items())[i][0]\n",
    "        if data_key == label_key:\n",
    "            filenames_list.append(data_key)\n",
    "        else:\n",
    "            raise Exception('data and labels are not sorted in sequence')\n",
    "        \n",
    "    return (data_list, labels_list, filenames_list)\n",
    "\n",
    "emails_list, categories_list, filenames_list = get_data_and_labels(sorted_emails, sorted_categories)\n",
    "assert len(emails_list) == len(categories_list)\n",
    "assert len(emails_list) == len(filenames_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def fit_tokenizer(texts, MAX_NB_WORDS=10000):\n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='\\\"\\'\\\\')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = fit_tokenizer(emails_list)\n",
    "assert tokenizer.document_count == len (emails_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_texts_to_sequences(texts):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return sequences\n",
    "\n",
    "sequences = convert_texts_to_sequences(emails_list)\n",
    "assert len(sequences) == len(emails_list)\n",
    "first_word_in_first_email = emails_list[0].split()[0].lower()\n",
    "assert sequences[0][0] == tokenizer.word_index[first_word_in_first_email]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence \n",
    "\n",
    "def zeropad_data(sequences, MAX_SEQUENCE_LENGTH=1000):\n",
    "    data = sequence.pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    return data\n",
    "\n",
    "MAX_SEQUENCE_LENGTH=500\n",
    "data = zeropad_data(sequences, MAX_SEQUENCE_LENGTH)\n",
    "assert data.shape == (len(sequences), MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def convert_labels_to_categorical_vector(y):\n",
    "    return MultiLabelBinarizer().fit_transform(y)\n",
    "\n",
    "labels = convert_labels_to_categorical_vector(categories_list)\n",
    "assert labels.shape == (len(categories_list), len(max(categories_list,key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, labels, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of emails in each category in training set:\n",
      "[625 503 603 134]\n",
      "Number of emails in each category in validation set:\n",
      "[209 174 201  44]\n"
     ]
    }
   ],
   "source": [
    "print('Number of emails in each category in training set:')\n",
    "print(y_train.sum(axis=0))\n",
    "print('Number of emails in each category in validation set:')\n",
    "print(y_val.sum(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           320000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 373,604\n",
      "Trainable params: 373,604\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "\n",
    "def build_lstm_classifier(max_sequence_length, no_of_output_labels):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=10000, output_dim=32, input_length=max_sequence_length))\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dense(no_of_output_labels, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "    \n",
    "model = build_lstm_classifier(MAX_SEQUENCE_LENGTH, len(max(categories_list,key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 625 samples, validate on 209 samples\n",
      "Epoch 1/3\n",
      "625/625 [==============================] - 13s - loss: 0.6724 - acc: 0.8292 - val_loss: 0.6033 - val_acc: 0.8959\n",
      "Epoch 2/3\n",
      "625/625 [==============================] - 15s - loss: 0.3803 - acc: 0.8888 - val_loss: 0.2923 - val_acc: 0.8959\n",
      "Epoch 3/3\n",
      "625/625 [==============================] - 15s - loss: 0.2981 - acc: 0.8888 - val_loss: 0.2893 - val_acc: 0.8959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1200abdd8>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "def calculate_precision_score(y_true, y_predicted):\n",
    "    return precision_score(y_true, y_predicted.round(), average='micro')\n",
    "\n",
    "expected = y_val\n",
    "predicted = model.predict(X_val)\n",
    "precision = calculate_precision_score(expected, predicted)\n",
    "assert precision > 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def calculate_recall_score(y_true, y_predicted):\n",
    "    return recall_score(y_true, y_predicted.round(), average='micro')\n",
    "\n",
    "recall = calculate_recall_score(expected, predicted)\n",
    "assert recall > 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tw-ml-template",
   "language": "python",
   "name": "tw-ml-template"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
